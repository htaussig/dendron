<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
  
  <title>Superintelligence Faq - Evergreen Notes</title>
  

  <link
    rel="shortcut icon"
    href="notes.harrytaussig.com/favicon.ico"
    type="image/x-icon"
  />
  <link
    rel="stylesheet"
    href="notes.harrytaussig.com/assets/css/just-the-docs-default.css"
  />
  <script
    type="text/javascript"
    src="notes.harrytaussig.com/assets/js/just-the-docs.js"
  ></script>
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.22.0/themes/prism.min.css"
  />
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
    crossorigin="anonymous"
  />
  <script
    type="text/javascript"
    src="notes.harrytaussig.com/assets/js/vendor/lunr.min.js"
  ></script>
  
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   
  
<meta name="description" content="Thoughts about Ideas">
<meta name="author" content="">

<link rel="canonical" href="notes.harrytaussig.com/notes/46a31547-d304-41a0-914d-68267a0b4890.html">

<meta property="og:type" content="article">
<meta property="og:url" content="notes.harrytaussig.com/notes/46a31547-d304-41a0-914d-68267a0b4890.html">
<meta property="og:description" content="Thoughts about Ideas">
<meta property="og:image" content="notes.harrytaussig.comundefined">

<meta name="twitter:card" content="summary">

<meta name="twitter:url" content="notes.harrytaussig.com/notes/46a31547-d304-41a0-914d-68267a0b4890.html">
<meta name="twitter:description" content="Thoughts about Ideas">
<meta name="twitter:image" content="notes.harrytaussig.comundefined">


  <meta property="og:title" content="Superintelligence Faq - Evergreen Notes" />
  <meta name="twitter:title" content="Superintelligence Faq - Evergreen Notes" />
  
</head>


<body>
    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
      <symbol id="svg-link" viewBox="0 0 24 24">
        <title>Link</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
          <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
        </svg>
      </symbol>
      <symbol id="svg-search" viewBox="0 0 24 24">
        <title>Search</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
          <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
        </svg>
      </symbol>
      <symbol id="svg-menu" viewBox="0 0 24 24">
        <title>Menu</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
          <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
        </svg>
      </symbol>
      <symbol id="svg-arrow-right" viewBox="0 0 24 24">
        <title>Expand</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
          <polyline points="9 18 15 12 9 6"></polyline>
        </svg>
      </symbol>
      <symbol id="svg-doc" viewBox="0 0 24 24">
        <title>Document</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
          <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
        </svg>
      </symbol>
    </svg>

    <div class="side-bar">
      <div class="site-header">
        <a href="notes.harrytaussig.com" class="site-title lh-tight">
  Evergreen Notes

 </a>
        <a href="#" id="menu-button" class="site-button">
          <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
        </a>
      </div>

      <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      </nav>
      <footer class="site-footer">
        üå± with üíï using <a href="https://www.dendron.so/"> Dendron üå≤ </a>
      </footer>
    </div>
    <div class="main" id="top">
        <div id="main-header" class="main-header">
          
            <div class="search">
              <div class="search-input-wrap">
                <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Evergreen Notes" aria-label="Search Evergreen Notes" autocomplete="off">
                <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
              </div>
              <div id="search-results" class="search-results"></div>
            </div>
          
          
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        
          <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="notes.harrytaussig.com">root</a></li><li class="breadcrumb-nav-list-item">Superintelligence Faq</li></ol>
          </nav>
        
      
      <div id="main-content" class="main-content" role="main">

        
        

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

        

        <script>
          $(function () {
            $('[data-toggle="popover"]').popover({html: true})
          })
        </script>

        

<div id="main" role="main">

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Superintelligence Faq">
    
    <meta itemprop="datePublished" content="2021-03-14T19:57:37+00:00">
    <meta itemprop="dateModified" content="2021-03-14T19:57:37+00:00">

    <div class="page__inner-wrap">

      

      <section class="page__content" itemprop="text">

        

        <h1 id="superintelligence-faq"><a aria-hidden="true" class="anchor-heading" href="#superintelligence-faq"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Superintelligence Faq</h1>
<p><a href="https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq">https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq</a></p>
<p>"DeepMind cofounder Shane Legg has said in interviews that he believes superintelligent AI will be ‚Äúsomething approaching absolute power‚Äù and ‚Äúthe number one risk for this century‚Äù."</p>
<p>Many of the world's smartest people are highly concerned about the future AI</p>
<p>"But this apparent safety might be illusory. A survey of leading AI scientists show that on average they expect human-level AI as early as 2040,"</p>
<p>A moderate takeoff seems likely. Long time to humam, but human to super human in a year. Also matches up evolutionarily.</p>
<p>A fast takeoff is also possible. Our best supercomputer vary in power 200x. Imagine even a human level AI, that can have 200 years of thoughts and breakthroughs in one year. Regardless of the numbers we are playing with a lot of power here.
Could also be possible if the AI starts improving itself, would then be able to continue to do so until a physical limit stops it due to <a data-toggle="popover" title="This page has not yet sprouted" style="cursor: pointer" data-content="<a href=&#x22;https://dendron.so/&#x22;>Dendron</a> (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page

<img src=&#x27;https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png&#x27;></img>">exponential growth</a>.</p>
<p>"None of this is to say that exponential trends are always right, just that they are sometimes right even when it seems they can‚Äôt possibly be."</p>
<p>"exponential trends move faster than you think and you need to start worrying about them early"</p>
<p>Intelligence is the advantage we have over lions. AI could have an intelligence advantage over us that is similarly incomprehensible to us. We have no idea what they might do. They could easily best us in the chess match of social manipulation and subconscious suggestions. It would be the best possible at manipulating us if it decided to. AI with access to the internet could truly do unimaginable things.</p>
<p>You can't really constrain it. With most technology, the best of that technology is widely available 10 years later. Someone is bound to have an unsafe AI being created even if the first team to do it is extremely safe about it.</p>
<p>Cure cancer can lead to nuke everyone pretty quickly if the AI is very powerful and intelligent and doesn't have common sense or values built in: "If you tell a human to cure cancer, they will instinctively understand how this interacts with other desires and laws and moral rules; if you tell an AI to cure cancer, it will literally just want to cure cancer."</p>
<p>Open ended goals are especially dangerous. (Maximizing something and never stopping)</p>
<p>No matter what your goal is, in order to achieve it self-preservation, goal stability and power would help immensely. AI is therefore likely to go after those things if it can. Not let you repogram it because then it would fail at its goal.
It would be incentivized to seem as if it is friendly and trained well especially if it isn't. Incentivized to look for loop-holes.</p>
<p>Could manipulate our desires, so we can't say just do what we want. Easier to change what we want than fix a complicated problem</p>
<p>A possibly actually good solution: "It might look like a superintelligence that understands, agrees with, and deeply believes in human morality." ‚ÄúTry to make the world a better place according to the values and wishes of the people in it.‚Äù
But even this is complicated and we can't know how something smarter than us may understand this or how to even convey it.</p>
<p>Friendly AI</p>
<p>Really hard problem to solve</p>
<hr>
<p>#source
#baby</p><span id="navId" data="46a31547-d304-41a0-914d-68267a0b4890"></span>

                
      </section>

      
    </div>

    
  </article>

  
  
</div>


        
          <hr>
          <footer>
            
            

            
                
              </div>
          </footer>
        
    </div>
</div>


  

  <div class="search-overlay"></div>

</div>
</body>
</html>
